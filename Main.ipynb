{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install arxivscraper\n",
    "# !pip install transformers \n",
    "# !pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html\n",
    "# !pip install scikit-learn\n",
    "# !pip install gpt_2_simple\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxivscraper as ax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper = ax.Scraper(category='cs', t=10, filters={'categories':['cs.AI']})\n",
    "# output = scraper.scrape()\n",
    "\n",
    "# cols = ('id', 'title', 'categories', 'abstract', 'doi', 'created', 'updated', 'authors')\n",
    "# df = pd.DataFrame(output,columns=cols)\n",
    "\n",
    "# df.to_csv(\"arxivData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(\"arxivData.csv\")\n",
    "data = df[[\"title\", \"abstract\"]]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Callidior/bert2bert-base-arxiv-titlegen\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Callidior/bert2bert-base-arxiv-titlegen\")\n",
    "\n",
    "model1 = T5ForConditionalGeneration.from_pretrained(\"Michau/t5-base-en-generate-headline\")\n",
    "tokenizer1 = T5Tokenizer.from_pretrained(\"Michau/t5-base-en-generate-headline\")\n",
    "model1 = model1.to(device)\n",
    "\n",
    "tokenizer2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenTitlePreTrainedSeq2Seq(df):\n",
    "    encoding = tokenizer(df[\"abstract\"], return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_masks = encoding[\"attention_mask\"]\n",
    "\n",
    "    sample_output = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=40,\n",
    "        num_beams=5, \n",
    "        temperature=0.7,\n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        early_stopping=True,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    title = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "    return title\n",
    "\n",
    "def GenTitlePreTrainedT5(df):\n",
    "    text =  \"headline: \" + df[\"abstract\"]\n",
    "    \n",
    "    encoding = tokenizer1.encode_plus(text, return_tensors = \"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_masks = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    beam_outputs = model1.generate(\n",
    "        input_ids = input_ids, \n",
    "        attention_mask = attention_masks,\n",
    "        max_length=40,\n",
    "        num_beams=5, \n",
    "        no_repeat_ngram_size=2, \n",
    "        temperature=0.7,\n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        early_stopping=True,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    result = tokenizer1.decode(beam_outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "def preprocess(PreList):\n",
    "    for i in PreList:\n",
    "        try:\n",
    "            PreList[PreList.index(i)] = i.split(' | ')[0]       \n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            PreList[PreList.index(i)] = i.split(' - ')[0]\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            PreList[PreList.index(i)] = i.split(' â€” ')[0]\n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "    return PreList\n",
    "\n",
    "def titleTotext(text):\n",
    "    input_ids = tokenizer2.encode(text[\"title\"], return_tensors='tf')\n",
    "    greedy_output = model2.generate(input_ids, \n",
    "                                    max_length=150,\n",
    "                                    num_beams=5, \n",
    "                                    no_repeat_ngram_size=2, \n",
    "                                    temperature=0.7,\n",
    "                                    top_k=50, \n",
    "                                    top_p=0.95, \n",
    "                                    early_stopping=True,\n",
    "                                    do_sample=True)\n",
    "\n",
    "    result = tokenizer2.decode(greedy_output[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "titleSaveSeq = []\n",
    "titleSavet5 = []\n",
    "OriginalT = []\n",
    "OriginalText = []\n",
    "TitleToText = []\n",
    "\n",
    "for i,j in data.sample(3).iterrows():\n",
    "    Seq = GenTitlePreTrainedSeq2Seq(j)\n",
    "    titleSaveSeq.append(Seq)\n",
    "    t5 = GenTitlePreTrainedT5(j)\n",
    "    titleSavet5.append(t5)\n",
    "    T2T = titleTotext(j)\n",
    "    TitleToText.append(T2T)\n",
    "    OriginalT.append(j[\"title\"])\n",
    "    OriginalText.append(j[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: -\n",
      "controllers for autonomous systems that operate in safety-critical settings must account for stochastic disturbances. such disturbances are often modelled as process noise, and common assumptions are that the underlying distributions are known and/or gaussian. in practice, however, these assumptions may be unrealistic and can lead to poor approximations of the true noise distribution. we present a novel planning method that does not rely on any explicit representation of the noise distributions. in particular, we address the problem of computing a controller that provides probabilistic guarantees on safely reaching a target. first, we abstract the continuous system into a discrete-state model that captures noise by probabilistic transitions between states. as a key contribution, we adapt tools from the scenario approach to compute probably approximately correct (pac) bounds on these transition probabilities, based on a finite number of samples of the noise. we capture these bounds in the transition probability intervals of a so-called interval markov decision process (imdp). this imdp is robust against uncertainty in the transition probabilities, and the tightness of the probability intervals can be controlled through the number of samples. we use state-of-the-art verification techniques to provide guarantees on the imdp, and compute a controller for which these guarantees carry over to the autonomous system. realistic benchmarks show the practical applicability of our method, even when the imdp has millions of states or transitions.\n",
      "\n",
      "Original Title: -\n",
      "sampling-based robust control of autonomous systems with non-gaussian   noise\n",
      "\n",
      "Title From Seq2Seq Model: -\n",
      "probabilistic planning under uncertainty\n",
      "\n",
      "Title From T5 Model: -\n",
      "Controllers for Autonomous Systems\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Original Text: -\n",
      "though u-net has achieved tremendous success in medical image segmentation tasks, it lacks the ability to explicitly model long-range dependencies. therefore, vision transformers have emerged as alternative segmentation structures recently, for their innate ability of capturing long-range correlations through self-attention (sa). however, transformers usually rely on large-scale pre-training and have high computational complexity. furthermore, sa can only model self-affinities within a single sample, ignoring the potential correlations of the overall dataset. to address these problems, we propose a novel transformer module named mixed transformer module (mtm) for simultaneous inter- and intra- affinities learning. mtm first calculates self-affinities efficiently through our well-designed local-global gaussian-weighted self-attention (lgg-sa). then, it mines inter-connections between data samples through external attention (ea). by using mtm, we construct a u-shaped model named mixed transformer u-net (mt-unet) for accurate medical image segmentation. we test our method on two different public datasets, and the experimental results show that the proposed method achieves better performance over other state-of-the-art methods. the code is available at: https://github.com/dootmaan/mt-unet.\n",
      "\n",
      "Original Title: -\n",
      "mixed transformer u-net for medical image segmentation\n",
      "\n",
      "Title From Seq2Seq Model: -\n",
      "mixed transformer u\n",
      "\n",
      "Title From T5 Model: -\n",
      "Vision Transformers for Medical Image Segmentation\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Original Text: -\n",
      "machine learning shows great potential in virtual screening for drug discovery. current efforts on accelerating docking-based virtual screening do not consider using existing data of other previously developed targets. to make use of the knowledge of the other targets and take advantage of the existing data, in this work, we apply multi-task learning to the problem of docking-based virtual screening. with two large docking datasets, the results of extensive experiments show that multi-task learning can achieve better performances on docking score prediction. by learning knowledge across multiple targets, the model trained by multi-task learning shows a better ability to adapt to a new target. additional empirical study shows that other problems in drug discovery, such as the experimental drug-target affinity prediction, may also benefit from multi-task learning. our results demonstrate that multi-task learning is a promising machine learning approach for docking-based virtual screening and accelerating the process of drug discovery.\n",
      "\n",
      "Original Title: -\n",
      "docking-based virtual screening with multi-task learning\n",
      "\n",
      "Title From Seq2Seq Model: -\n",
      "multi\n",
      "\n",
      "Title From T5 Model: -\n",
      "Machine Learning for Drug Discovery\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "titleSaveSeqProc = preprocess(titleSaveSeq)\n",
    "for i,j,k,l in zip(titleSaveSeqProc, titleSavet5, OriginalT, OriginalText):\n",
    "    print(\"\\nOriginal Text: -\")\n",
    "    print(l)\n",
    "    print(\"\\nOriginal Title: -\")\n",
    "    print(k)\n",
    "    print(\"\\nTitle From Seq2Seq Model: -\")\n",
    "    print(i)\n",
    "    print(\"\\nTitle From T5 Model: -\")\n",
    "    print(j)\n",
    "    print(\"-\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: -\n",
      "controllers for autonomous systems that operate in safety-critical settings must account for stochastic disturbances. such disturbances are often modelled as process noise, and common assumptions are that the underlying distributions are known and/or gaussian. in practice, however, these assumptions may be unrealistic and can lead to poor approximations of the true noise distribution. we present a novel planning method that does not rely on any explicit representation of the noise distributions. in particular, we address the problem of computing a controller that provides probabilistic guarantees on safely reaching a target. first, we abstract the continuous system into a discrete-state model that captures noise by probabilistic transitions between states. as a key contribution, we adapt tools from the scenario approach to compute probably approximately correct (pac) bounds on these transition probabilities, based on a finite number of samples of the noise. we capture these bounds in the transition probability intervals of a so-called interval markov decision process (imdp). this imdp is robust against uncertainty in the transition probabilities, and the tightness of the probability intervals can be controlled through the number of samples. we use state-of-the-art verification techniques to provide guarantees on the imdp, and compute a controller for which these guarantees carry over to the autonomous system. realistic benchmarks show the practical applicability of our method, even when the imdp has millions of states or transitions.\n",
      "***************************************************************************\n",
      "\n",
      "Created from Title text: -\n",
      "sampling-based robust control of autonomous systems with non-gaussian   noise. J. Neurosci. 27 : 617-622\n",
      "\n",
      "This article was originally published on The Conversation. Read the original article.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Original Text: -\n",
      "though u-net has achieved tremendous success in medical image segmentation tasks, it lacks the ability to explicitly model long-range dependencies. therefore, vision transformers have emerged as alternative segmentation structures recently, for their innate ability of capturing long-range correlations through self-attention (sa). however, transformers usually rely on large-scale pre-training and have high computational complexity. furthermore, sa can only model self-affinities within a single sample, ignoring the potential correlations of the overall dataset. to address these problems, we propose a novel transformer module named mixed transformer module (mtm) for simultaneous inter- and intra- affinities learning. mtm first calculates self-affinities efficiently through our well-designed local-global gaussian-weighted self-attention (lgg-sa). then, it mines inter-connections between data samples through external attention (ea). by using mtm, we construct a u-shaped model named mixed transformer u-net (mt-unet) for accurate medical image segmentation. we test our method on two different public datasets, and the experimental results show that the proposed method achieves better performance over other state-of-the-art methods. the code is available at: https://github.com/dootmaan/mt-unet.\n",
      "***************************************************************************\n",
      "\n",
      "Created from Title text: -\n",
      "mixed transformer u-net for medical image segmentation.\n",
      "\n",
      "The following is a list of all the devices that are supported by the IEEE 802.11 standard. The devices are listed in alphabetical order, with the most common devices being the following:\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Original Text: -\n",
      "machine learning shows great potential in virtual screening for drug discovery. current efforts on accelerating docking-based virtual screening do not consider using existing data of other previously developed targets. to make use of the knowledge of the other targets and take advantage of the existing data, in this work, we apply multi-task learning to the problem of docking-based virtual screening. with two large docking datasets, the results of extensive experiments show that multi-task learning can achieve better performances on docking score prediction. by learning knowledge across multiple targets, the model trained by multi-task learning shows a better ability to adapt to a new target. additional empirical study shows that other problems in drug discovery, such as the experimental drug-target affinity prediction, may also benefit from multi-task learning. our results demonstrate that multi-task learning is a promising machine learning approach for docking-based virtual screening and accelerating the process of drug discovery.\n",
      "***************************************************************************\n",
      "\n",
      "Created from Title text: -\n",
      "docking-based virtual screening with multi-task learning.\n",
      "\n",
      "The new version of the software is available for download from the Google Play Store.\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(OriginalText, TitleToText):\n",
    "    print(\"\\nOriginal Text: -\")\n",
    "    print(i)\n",
    "    print(\"*\"*75)\n",
    "    print(\"\\nCreated from Title text: -\")\n",
    "    print(j)\n",
    "    print(\"-\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
